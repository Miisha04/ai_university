import numpy as np


# Обучить нейронную сеть, определяющую, будет ли человек

# удовлетворён своим отпуском. Входные данные: ранее посе-
# щённые места, интересы и бюджет
# обьем = 15
#act function is логистическая функция.


def f(x):
    return 1/(1 + np.exp(-x))

# функция для вычисления производной гиперболического тангенса
def df(x):
    return x*(1 - x)

# инициализация весов НС – случайные значения для 1-го слоя!!!!!
W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])
# инициализация весов НС – случайные значения для 2-го слоя
W2 = np.array([0.2, 0.3])

# функция, пропускающая вектор наблюдений через НС
def go_forward(inp):
    sum = np.dot(W1, inp)
    # сохранение выходных значений для каждого нейрона 1-го скрытого слоя
    out = np.array([f(x) for x in sum])

    sum = np.dot(W2, out)
    # сохранение выходного значения для всей НС (для последнего нейрона)
    y = f(sum)
    # возвращение выходных значений в виде кортежа
    return (y, out)

# функция обучения НС
def train(epoch):
    global W2, W1
    lmd = 0.3
         # шаг обучения!!!! 2
    N = 15000           # число итераций при обучении!!!!3
    # вычисление размера обучающей выборки
    count = len(epoch)
# итерации
    for k in range(N):
        # случайный выбор входного сигнала из обучающей выборки
        x = epoch[np.random.randint(0, count)]
        # прямой проход по НС и вычисление выходных значений нейронов
        y, out = go_forward(x[0:3])
        e = y - x[-1]                           # ошибка: y – требуемое значение
        delta = e*df(y)                         # локальный градиент
        # корректировка весов последнего слоя
        W2[0] = W2[0] - lmd * delta * out[0]    # корректировка веса первой связи
        W2[1] = W2[1] - lmd * delta * out[1]    # корректировка веса второй связи
        # вычисление локальных градиентов для нейронов скрытого слоя
        # вектор из 2-х величин локальных градиентов
        delta2 = W2*delta*df(out)
        # корректировка входных весов для нейронов первого скрытого слоя
        # каждый нейрон имеет 3 входящих связи, которые подлежат корректировке
        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd
        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd


# обучающая выборка (она же полная выборка)
#места / интересы / бюджет удволетворен
epoch = [
    (0.7, 0.3, 0.7,  0),
    (0.7, 0.3, 0.9,  0),
    (0.6, 0.8, 0.7,  1),
    (0.2, 0.2, 0.3,  0),
    (0.8, 0.6, 0.6,  1),
    (0.2, 0.1, 0.8,  0),
    (0.4, 0.2, 0.4,  0),
    (0.9, 0.7, 0.7,  1),
    (0.5, 0.1, 0.2,  0),
    (0.1, 0.6, 0.7,  1),
    (0.5, 0.7, 0.4,  0),
    (0.1, 0.1, 0.2,  0),
    (0.8, 0.6, 0.8,  1),
    (0.8, 0.6, 0.6,  1),
    (0.7, 0.1, 0.7,  0),
    (0.2, 0.2, 0.3, 1),
    (0.3, 0.3, 0.3, 1),
    (0.4, 0.4, 0.8, 1),
    (0.4, 0.5, 0.5, 1),
    (0.7, 0.6, 0.3, 0),
    (0.8, 0.4, 0.8, 0),
    (0.4, 0.9, 0.2, 0),
    (0.8, 0.8, 0.3, 0),
    (0.3, 0.6, 0.1, 0),
    (0.6, 0.5, 0.9, 1),
    (0.7, 0.7, 0.1, 0),
    (0.3, 0.9, 0.9, 1),
    (0.4, 0.9, 0.1, 0),
    (0.3, 0.9, 0.9, 1)
]


#исходные весовые коэффициенты скрытого слоя
print(f"Веса скрытого слоя: \n{W1}")

#исходные весовые коэффициенты выходного слоя
print(f"Веса скрытого слоя: {W2} \n")

train(epoch)        # запуск обучения сети

#откорректированные весовые коэффициенты скрытого слоя
print(f"Откорректированные веса скрытого слоя: \n{W1}")

#откорректированные весовые коэффициенты выходного слоя
print(f"Откорректированные веса скрытого слоя: {W2} \n")

# проверка полученных результатов на каждом наборе данных обучающей выборки
for x in epoch:
    y, out = go_forward(x[0:3])
    print(f"Выходное значение НС для входного набора {x[0:3]}: {y} => {x[-1]}")